import torch
import cv2
import os
from ultralytics import YOLO
from image_model.config import *  # CLASS_LABELS, CLS_MODEL_PATH, COLOR, BLOCKLIST, CLS_CONF_THRESHOLD ë“±
import open_clip
from PIL import ImageFont, ImageDraw, Image
import numpy as np
from torchvision import transforms
import config
from utils.emoji_mapper import get_korean_name, get_english_label


def is_korean_text(text):
    """í…ìŠ¤íŠ¸ê°€ í•œê¸€ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸"""
    for char in text:
        if '\uAC00' <= char <= '\uD7AF':  # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„
            return True
    return False


def find_working_font(font_paths, font_size=12):
    """ì‘ë™í•˜ëŠ” í°íŠ¸ë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
    for font_path in font_paths:
        try:
            if os.path.exists(font_path):
                font = ImageFont.truetype(font_path, font_size)
                # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ í°íŠ¸ ê²€ì¦
                test_text = "ê°€ë‚˜ë‹¤ë¼"
                bbox = font.getbbox(test_text)
                if bbox[2] > 0 and bbox[3] > 0:  # ìœ íš¨í•œ ë°”ìš´ë”© ë°•ìŠ¤
                    print(f"âœ… í°íŠ¸ ê²€ì¦ ì„±ê³µ: {font_path}")
                    return font
        except Exception as e:
            print(f"âš ï¸ í°íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {font_path}, ì˜¤ë¥˜: {e}")
            continue
    return None


# íŒŒì¼ ìƒë‹¨(ìµœì´ˆ 1íšŒë§Œ ë¡œë”©)


def draw_labeled_box(image: np.ndarray, bbox: list[int], label: str, color=COLOR, font_size=12):
    """
    ì´ë¯¸ì§€ì— ë¼ë²¨ê³¼ ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
    :param image: BGR ì´ë¯¸ì§€ (OpenCV)
    :param bbox: [x1, y1, x2, y2] ì¢Œí‘œ
    :param label: í‘œì‹œí•  í…ìŠ¤íŠ¸
    :param color: ë°•ìŠ¤ ìƒ‰ìƒ (BGR)
    :param font_size: í…ìŠ¤íŠ¸ í°íŠ¸ í¬ê¸°
    :return: ë°•ìŠ¤ì™€ í…ìŠ¤íŠ¸ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€
    """
    x1, y1, x2, y2 = bbox
    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)

    # OpenCV â†’ PIL ë³€í™˜
    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(image_pil)

    # í•œê¸€ í°íŠ¸ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
    korean_font_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        "/System/Library/Fonts/Arial.ttf",  # macOS
        "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
    ]
    
    # í…ìŠ¤íŠ¸ê°€ í•œê¸€ì¸ì§€ í™•ì¸
    is_korean = is_korean_text(label)
    
    if is_korean:
        # í•œê¸€ í°íŠ¸ ì°¾ê¸°
        font = find_working_font(korean_font_paths, font_size)
        if font is None:
            print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
            font = ImageFont.load_default()
    else:
        # ì˜ì–´ í…ìŠ¤íŠ¸ëŠ” ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
        font = ImageFont.load_default()

    # í…ìŠ¤íŠ¸ ë Œë”ë§ ì‹œë„
    try:
        draw.text((x1, y1 - font_size - 2), label, font=font, fill=(255, 0, 0))
        if is_korean:
            print(f"âœ… í•œê¸€ í…ìŠ¤íŠ¸ ë Œë”ë§ ì„±ê³µ: {label}")
        else:
            print(f"âœ… ì˜ì–´ í…ìŠ¤íŠ¸ ë Œë”ë§ ì„±ê³µ: {label}")
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ë Œë”ë§ ì‹¤íŒ¨: {label}, ì˜¤ë¥˜: {e}")
        if is_korean:
            # í•œê¸€ ë Œë”ë§ ì‹¤íŒ¨ ì‹œ ì˜ì–´ë¡œ ëŒ€ì²´
            english_label = get_english_label(label)
            draw.text((x1, y1 - font_size - 2), english_label, font=font, fill=(255, 0, 0))
            print(f"ğŸ”„ ì˜ì–´ë¡œ ëŒ€ì²´: {label} â†’ {english_label}")
        else:
            # ì˜ì–´ë„ ì‹¤íŒ¨í•˜ë©´ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            simple_label = label[:10] if len(label) > 10 else label
            draw.text((x1, y1 - font_size - 2), simple_label, font=font, fill=(255, 0, 0))
            print(f"ğŸ”„ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´: {label} â†’ {simple_label}")

    # PIL â†’ OpenCV ë³€í™˜
    return cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)


def get_class_info(label):
    """YOLO class labelì„ í•œê¸€ê³¼ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜"""
    return CLASS_MAP.get(label.lower(), ("ì•Œ ìˆ˜ ì—†ìŒ", "ê¸°íƒ€"))


def classify_yolocls(image_path, keep, all_boxes, all_crops):
    """YOLO ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ë¡œ crop ì´ë¯¸ì§€ ë¶„ë¥˜"""
    image = cv2.imread(image_path)
    cls_model = YOLO(YOLO_CLS_MODEL_PATH)
    detections = []

    for i in keep:
        idx = int(i.item()) if isinstance(i, torch.Tensor) else int(i)
        x1, y1, x2, y2 = map(int, all_boxes[idx])
        crop = all_crops[idx]

        cls_result = cls_model(crop)[0]
        if not hasattr(cls_result, "probs"):
            continue

        cls_id = int(cls_result.probs.top1)
        cls_label = cls_result.names[cls_id]
        cls_conf = float(cls_result.probs.data[cls_id])

        # BLOCKLIST ì¡°ê±´ë¶€ í•„í„°ë§: ë¸”ë¡ë¦¬ìŠ¤íŠ¸ ë¼ë²¨ì€ conf 0.95 ì´ìƒì¼ ë•Œë§Œ í—ˆìš©
        if cls_label in BLOCKLIST and cls_conf < 0.95:
            continue
        if cls_conf < CLS_CONF_THRESHOLD:
            continue

        kor_label, category = get_class_info(cls_label)
        
        # í•œê¸€ ë¼ë²¨ë¡œ ë³€í™˜ (emojiMap ê¸°ì¤€)
        korean_label = get_korean_name(cls_label)
        
        # box ë° label ì´ë¯¸ì§€ì— í‘œì‹œ (í•œê¸€ ë¼ë²¨ ì‚¬ìš©)
        image = draw_labeled_box(image=image,bbox=[x1,y1,x2,y2],label=korean_label)
        
        detections.append({
            "label": cls_label,
            "korean": korean_label,
            "category": category,
            "conf": round(cls_conf, 3),
            "bbox": [x1, y1, x2, y2]
        })

    return detections, image


def load_finetuned_clip(model_path, device="cuda"):
    model_name = "ViT-L-14"
    pretrained = "openai"
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained, device=device
    )
    if os.path.exists(model_path):
        print('using pretrained model')
        model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    tokenizer = open_clip.get_tokenizer(model_name)
    return model, preprocess, tokenizer



def get_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_path = os.path.join(PRETRAINED_FOLDER, "clip", CLIP_PRETRAINED)
    clip_model, preprocess, tokenizer = load_finetuned_clip(model_path, device=device)
    return clip_model, preprocess, tokenizer


##### open_clip ì‚¬ìš©í•˜ì—¬ fine-tune ì‹œë„
def classify_clip(image_path, keep, all_boxes, all_crops):
    """CLIP ëª¨ë¸ì„ ì´ìš©í•œ ììœ  ë¼ë²¨ ê¸°ë°˜ ë¶„ë¥˜"""
    import time
    image = cv2.imread(image_path)
    detections = []
    device = "cuda" if torch.cuda.is_available() else "cpu"

    clip_model, preprocess, tokenizer = config.clip_model, config.preprocess, config.tokenizer
    
    # --- 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ---
    t1 = time.time()
    text_prompts = [f"A photo of {label.replace('_', ' ')}" for label in CLASS_LABELS]
    text_tokens = tokenizer(text_prompts).to(device)
    with torch.no_grad():
        text_features = clip_model.encode_text(text_tokens)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    t2 = time.time()
    print(f"[TIME] í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: {t2-t1:.3f}ì´ˆ")

    # --- 3. cropë³„ ë£¨í”„ ì‹œì‘ ì „ ---
    t3 = time.time()
    print(f"[TIME] cropë³„ ë¶„ë¥˜ ë£¨í”„ ì§„ì…: {t3-t2:.3f}ì´ˆ")

    # --- ì „ì²´ íƒ€ì´ë¨¸ ì‹œì‘ ---
    start_time = time.time()

    for i, box_idx in enumerate(keep):
        crop_start = time.time()
        idx = int(box_idx.item()) if isinstance(box_idx, torch.Tensor) else int(box_idx)
        x1, y1, x2, y2 = map(int, all_boxes[idx])
        crop = all_crops[idx]

        crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
        crop_input = preprocess(crop_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            image_feature = clip_model.encode_image(crop_input)
            image_feature /= image_feature.norm(dim=-1, keepdim=True)
            similarity = (100.0 * image_feature @ text_features.T).softmax(dim=-1)

        cls_id = int(similarity.argmax().item())
        cls_label = CLASS_LABELS[cls_id]
        cls_conf = float(similarity[0, cls_id])

        # WHITELIST_MAP í‘œì¤€í™” ì ìš©
        from image_model.config import WHITELIST_MAP
        if cls_label in WHITELIST_MAP:
            std_label = WHITELIST_MAP[cls_label]
            print(f"[WHITELIST í‘œì¤€í™”] {cls_label} â†’ {std_label}")
            cls_label = std_label

        # BLOCKLIST ì²´í¬ í”„ë¦°íŠ¸
        print(f"[BLOCKLIST ì²´í¬] label: {cls_label}, conf: {cls_conf:.3f}, BLOCKLIST: {cls_label in BLOCKLIST}")

        # BLOCKLIST ì¡°ê±´ë¶€ í•„í„°ë§: ë¸”ë¡ë¦¬ìŠ¤íŠ¸ ë¼ë²¨ì€ conf 0.9 ì´ìƒì¼ ë•Œë§Œ í—ˆìš©
        if cls_label in BLOCKLIST and cls_conf < 0.7:
            continue
        if cls_conf < CLS_CONF_THRESHOLD:
            continue

        # í•œê¸€ ë¼ë²¨ë¡œ ë³€í™˜
        korean_label = get_korean_name(cls_label)

        # box ë° label ì´ë¯¸ì§€ì— í‘œì‹œ (í•œê¸€ ë¼ë²¨ ì‚¬ìš©)
        image = draw_labeled_box(image=image,bbox=[x1,y1,x2,y2],label=korean_label)

        detections.append({
            "label": cls_label,
            "korean": korean_label,
            "category": 'clip',
            "conf": round(cls_conf, 3),
            "bbox": [x1, y1, x2, y2]
        })
        # CLIP ê²°ê³¼ í”„ë¦°íŠ¸
        print(f"[CLIP ê²°ê³¼] label: {cls_label}, conf: {cls_conf:.3f}, bbox: {[x1, y1, x2, y2]}")
        crop_elapsed = time.time() - crop_start
        print(f"[CLIP ë¶„ë¥˜] crop {i+1}/{len(keep)}: {crop_elapsed:.3f}ì´ˆ")

    # --- ì „ì²´ íƒ€ì´ë¨¸ ë ---
    elapsed = time.time() - start_time
    print(f"[CLIP ë¶„ë¥˜ ì†Œìš” ì‹œê°„] {elapsed:.3f}ì´ˆ (YOLO ë°•ìŠ¤ ì´í›„ ~ CLIP ë¶„ë¥˜ ì „ì²´)")

    return detections, image


def classify_clip_filtered_bbox(image_path, keep, all_boxes, all_crops, confidence_threshold=0.7):
    """CLIP ëª¨ë¸ì„ ì´ìš©í•œ ììœ  ë¼ë²¨ ê¸°ë°˜ ë¶„ë¥˜ (ì •í™•ë„ 70% ë¯¸ë§Œë§Œ bounding box í‘œì‹œ)"""
    import time
    image = cv2.imread(image_path)
    detections = []
    device = "cuda" if torch.cuda.is_available() else "cpu"

    clip_model, preprocess, tokenizer = config.clip_model, config.preprocess, config.tokenizer
    
    # --- 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ---
    t1 = time.time()
    text_prompts = [f"A photo of {label.replace('_', ' ')}" for label in CLASS_LABELS]
    text_tokens = tokenizer(text_prompts).to(device)
    with torch.no_grad():
        text_features = clip_model.encode_text(text_tokens)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    t2 = time.time()
    print(f"[TIME] í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: {t2-t1:.3f}ì´ˆ")

    # --- 3. cropë³„ ë£¨í”„ ì‹œì‘ ì „ ---
    t3 = time.time()
    print(f"[TIME] cropë³„ ë¶„ë¥˜ ë£¨í”„ ì§„ì…: {t3-t2:.3f}ì´ˆ")

    # --- ì „ì²´ íƒ€ì´ë¨¸ ì‹œì‘ ---
    start_time = time.time()

    for i, box_idx in enumerate(keep):
        crop_start = time.time()
        idx = int(box_idx.item()) if isinstance(box_idx, torch.Tensor) else int(box_idx)
        x1, y1, x2, y2 = map(int, all_boxes[idx])
        crop = all_crops[idx]

        crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
        crop_input = preprocess(crop_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            image_feature = clip_model.encode_image(crop_input)
            image_feature /= image_feature.norm(dim=-1, keepdim=True)
            similarity = (100.0 * image_feature @ text_features.T).softmax(dim=-1)

        cls_id = int(similarity.argmax().item())
        cls_label = CLASS_LABELS[cls_id]
        cls_conf = float(similarity[0, cls_id])

        # WHITELIST_MAP í‘œì¤€í™” ì ìš©
        from image_model.config import WHITELIST_MAP
        if cls_label in WHITELIST_MAP:
            std_label = WHITELIST_MAP[cls_label]
            print(f"[WHITELIST í‘œì¤€í™”] {cls_label} â†’ {std_label}")
            cls_label = std_label

        # BLOCKLIST ì²´í¬ í”„ë¦°íŠ¸
        print(f"[BLOCKLIST ì²´í¬] label: {cls_label}, conf: {cls_conf:.3f}, BLOCKLIST: {cls_label in BLOCKLIST}")

        # BLOCKLIST ì¡°ê±´ë¶€ í•„í„°ë§: ë¸”ë¡ë¦¬ìŠ¤íŠ¸ ë¼ë²¨ì€ conf 0.9 ì´ìƒì¼ ë•Œë§Œ í—ˆìš©
        if cls_label in BLOCKLIST and cls_conf < 0.7:
            continue
        if cls_conf < CLS_CONF_THRESHOLD:
            continue

        # í•œê¸€ ë¼ë²¨ë¡œ ë³€í™˜
        korean_label = get_korean_name(cls_label)
        
        # ì •í™•ë„ 70% ë¯¸ë§Œì´ê³  18% ì´ìƒì¸ ê²½ìš°ì—ë§Œ bounding box ê·¸ë¦¬ê¸°
        if cls_conf < confidence_threshold and cls_conf >= 0.18:
            # ì •í™•ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ê²°ì •
            if cls_conf >= 0.3:
                color = (0, 165, 255)  # ì£¼í™©ìƒ‰ (30-70%)
            else:
                color = (0, 0, 255)    # ë¹¨ê°„ìƒ‰ (18-30%)
            
            # box ë° label ì´ë¯¸ì§€ì— í‘œì‹œ (í•œê¸€ ë¼ë²¨ ì‚¬ìš©, ìƒ‰ìƒ ì§€ì •)
            image = draw_labeled_box(image=image, bbox=[x1,y1,x2,y2], label=korean_label, color=color)

        detections.append({
            "label": cls_label,
            "korean": korean_label,
            "category": 'clip',
            "conf": round(cls_conf, 3),
            "bbox": [x1, y1, x2, y2]
        })
        # CLIP ê²°ê³¼ í”„ë¦°íŠ¸
        print(f"[CLIP ê²°ê³¼] label: {cls_label}, conf: {cls_conf:.3f}, bbox: {[x1, y1, x2, y2]}")
        crop_elapsed = time.time() - crop_start
        print(f"[CLIP ë¶„ë¥˜] crop {i+1}/{len(keep)}: {crop_elapsed:.3f}ì´ˆ")

    # --- ì „ì²´ íƒ€ì´ë¨¸ ë ---
    elapsed = time.time() - start_time
    print(f"[CLIP ë¶„ë¥˜ ì†Œìš” ì‹œê°„] {elapsed:.3f}ì´ˆ (YOLO ë°•ìŠ¤ ì´í›„ ~ CLIP ë¶„ë¥˜ ì „ì²´)")
    
    # bounding box ê·¸ë¦¬ê¸° ê²°ê³¼ ìš”ì•½
    bbox_drawn_count = 0
    for detection in detections:
        if detection['conf'] < confidence_threshold and detection['conf'] >= 0.18:
            bbox_drawn_count += 1
    
    print(f"[BOUNDING BOX ìš”ì•½] ì „ì²´ detections: {len(detections)}, bounding box ê·¸ë ¤ì§„ ê°œìˆ˜: {bbox_drawn_count}")
    print(f"[BOUNDING BOX ìš”ì•½] confidence_threshold: {confidence_threshold}, ìµœì†Œ ì„ê³„ê°’: 0.18")
    
    if bbox_drawn_count == 0:
        print(f"âš ï¸ [BOUNDING BOX ê²½ê³ ] ê·¸ë ¤ì§„ bounding boxê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ê°€ ì›ë³¸ê³¼ ë™ì¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    return detections, image


def classify_resnet(image_path, keep, all_boxes, all_crops):
    # read img
    image = cv2.imread(image_path)
    
    # preprocessing before classification 
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)])
    
    # load classifier
    device = "cuda" if torch.cuda.is_available() else "cpu"
    classifiers = {}
    for key, item in CLASSIFIER_MODELS.items():
        model = torch.load(os.path.join(PRETRAINED_FOLDER,'resnet',item['path']), map_location=device)
        model.eval()
        classifiers[key] = {'model': model, 'offset': item['class_offset']}
        
        
    detections = []
    for i in keep:
        idx = int(i.item()) if isinstance(i, torch.Tensor) else int(i)
        x1, y1, x2, y2 = map(int, all_boxes[idx])
        crop = all_crops[idx]
        
        inputs = transform(crop).unsqueeze(0).to(device)
        best_class, best_logit = None, float('-inf')

        with torch.no_grad():
            for key, info in classifiers.items():
                model = info['model']
                logits = model(inputs)[0]  # softmax ì—†ì´ logit ê·¸ëŒ€ë¡œ ì‚¬ìš©
                top1_idx = torch.argmax(logits).item()
                top1_logit = logits[top1_idx].item()
                global_class = info['offset'] + top1_idx

                if top1_logit > best_logit:
                    best_class = global_class
                    best_logit = top1_logit
        
        predicted_class = best_class
        class_name = CLASS_NAME_MAP.get(predicted_class, "ì•Œìˆ˜ì—†ìŒ")
        cls_label = f"{class_name} (logit:{best_logit:.2f})"

        # ì‹œê°í™”
        image = draw_labeled_box(image=image,bbox=[x1,y1,x2,y2],label=cls_label)

        detections.append({
            "label": cls_label,
            "korean": cls_label,
            "category": 'resnet',
            "conf": round(best_logit, 3),
            "bbox": [x1, y1, x2, y2]
        })

    return detections, image
        
    
    
    